---
description: "This page is Zhifeng's Study Diary of Week 1, December 2024."
long_title: "Week 1 - Dec 2024 - Study Diary - Zhifeng"
---

# Thursday

I have spent some time implementing my design of a "graph group" that can present multiple graphs in one scene. I verified that my idea of using a "path" to identify and only use a part of a JSON file is feasible. That's a reflection of the idea of "spatial locality" from UIUC's CS233 course.

# Friday, Dec. 6th

I have spent some time adjusting the links in the content part of my tome pages to target "\_blank" and understand how the header attributes in Markdown work. An interesting bug has happened: my "mdtome" uses the file system's modified time to track which files I need to rerender, and that turns out to be the reason why I didn't see the updates in code logic reflect on the resulting HTML: forgeting to "force" update.

## Reading _AI as a Resource_ { #reading_ai_as_a_resource }

> Donahue, K. (2024). AI as a Resource: Strategy, Uncertainty, and Societal Welfare (Doctoral dissertation, Cornell University). Link to her Personal Website: https://www.katedonahue.me/

> "The goal of my research is to view artificial intelligence as a societal resource used by self-interested agents who are often operating under some sources of uncertainty."

> "This framework models a setting where multiple self-interested agents each are trying to build a model that performs well on their own local distribution (which typically differs from the distributions of other agents, modeling the non-i.i.d setting)."

I wonder if we can model the "privileged bias" in similar ways. For example, if the population distribution of something is gradually changing with time, will the teacher models(who learned the parameters from previous distribution)'s estimation of the current students' learning be inaccurate? to what extent?, how different problems, like math education vs. economics vs. computer science affect the level of inaccuracy.

I haven't completely and carefully read the entire paper yet, but I'm curious about whether these distributions also apply to game learning agents with reinforcement learning. For example, if a chess master wants to hold his position, should they try to misguide their opponents with some weird gameplay "rewards," like trying to lead other players in the wrong exploring direction?

## Leetcode 2554. Day 852

I was in a relatively bad mood (the absolute magnitude is low, but still bad). You might be able to see that from my codes. I'm not carefully thinking about the suffix of my variables. I'm grateful that this piece of code passed the tests after two attempts.

```rust
pub fn max_count(mut banned: Vec<i32>, n: i32, max_sum: i32) -> i32 {
    banned.push(n + 1);
    banned.sort_unstable();
    let mut prev_banned_num = 0;
    let mut curr_sum = 0;
    let mut ans_count = 0;
    for banned_num in banned.into_iter() {
        // println!("{} {}", curr_sum, banned_num);
        if banned_num == prev_banned_num {
            continue;
        }
        let elem_num = banned_num - prev_banned_num - 1;
        let group_add_num = (prev_banned_num + banned_num) * (elem_num) / 2;
        if curr_sum + group_add_num > max_sum {
            for elem in (prev_banned_num + 1)..banned_num {
                if curr_sum + elem > max_sum {
                    return ans_count;
                }
                curr_sum += elem;
                ans_count += 1;
            }
        } else if curr_sum + group_add_num == max_sum {
            return ans_count + elem_num;
        }
        ans_count += elem_num;
        curr_sum += group_add_num;
        prev_banned_num = banned_num;
        if banned_num == n + 1 {
            return ans_count;
        }
    }
    ans_count
}
```

# Saturday

https://www.howtogeek.com/microsoft-word-change-quote-style/
