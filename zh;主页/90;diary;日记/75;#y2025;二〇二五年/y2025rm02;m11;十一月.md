---
description: "二〇二五年十一月"
long_title: "二〇二五年十一月 - 学习日记 - 之枫"
---

# 十一月阅读

| 发表年份 | 标题                                                                    | 短记                                                                         |
| -------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| 2016 年  | Context Encoders: Feature Learning by Inpainting                        | 用 encoder, decoder, 和 adversarial loss 去让程序发挥"创意"填充一些图形填空. |
| 2016 年  | 女人对家庭主妇对应男人对计算机编程员? 去除 Word Embeddings 中的性别歧视 | 去除 vector word embeddings 中的性别歧视.                                    |

# 学习日记

## 十一月七日礼拜五 { #y2025m11d07 }

我计划了很久在周五这天最后尝试一下上周就差一点点就满分的 CS440 MP8 Hidden Markov Models. 我想唯一可能和老师的文档有出入的地方就在于我一直纠结于把对应每一个前一个"token"的 conditional probability 的加法和通过一些操作变成一. 教授说其实不用太纠结这个问题, 只要把一些没见过的情况的概率设置成一个比较小的数字就可以了. 我之所以纠结这个的原因是在于我希望我可以直接使用方括号去访问字典数据结构里的内容, 这样就不用额外检查是否字典数据结构里并没有这个情况再额外做一个判断. 所以之前我就尝试把每种可能的情况都先填进字典数据结构, 并在这个过程中通过调整那个填充的小 epsilon 去让每一行, 也就是对于所有的下一个 token 给出前面见到的 token 后下一个 token 出现的概率的加法和为一. 我最终还是放弃了这个执念, 尽管当时我的模型就差 0.001 就达标了. 我很快就改完了之前的设计并提交了. 我盯着提交界面的窗口, 心里想如果要是真的是因为这个原因, 而我还一直有预感然后这次就通过了的话也是挺神奇的. 然后这个想法刚出现就看到屏幕显示这次提交顺利通过了, 感觉很惊喜, 虽然也不能完全确定是这个原因.
